#!/usr/bin/env python

import argparse
import fileinput
import re
import sys

xml_prefix = """<cib crm_feature_set="3.0.7" validate-with="pacemaker-1.2" admin_epoch="%d" epoch="0" num_updates="0">
  <!--
  ###########################################################################
  # $URL$
  # $Author$
  # $Date$
  # $Rev$
  ###########################################################################
  -->
  <configuration>
   <crm_config>
      <cluster_property_set id="cib-bootstrap-options">
        <nvpair id="cib-bootstrap-options-have-watchdog" name="have-watchdog" value="false"/>
        <nvpair id="cib-bootstrap-options-dc-version" name="dc-version" value="1.1.13-10.el7_2.2-44eb2dd"/>
        <nvpair id="cib-bootstrap-options-cluster-infrastructure" name="cluster-infrastructure" value="corosync"/>
        <nvpair id="cib-bootstrap-options-cluster-name" name="cluster-name" value="%s"/>
        <nvpair id="cib-bootstrap-options-stonith-enabled" name="stonith-enabled" value="true"/>
        <nvpair id="cib-bootstrap-options-symmetric-cluster" name="symmetric-cluster" value="false"/>
      </cluster_property_set>
   </crm_config>
   <nodes/>
"""

xml_resources = ""
xml_constraints = ""

xml_postfix = """  </configuration>
  <status/>
</cib>
"""

zpools_seen = {}

all_nodes = set()

def record_xml(nodes, lustre_service_name, zfs_dataset):
    """Record Pacemaker cib xml components for a dataset in global variables

    'nodes' is a list of node names in in priority order (highest to lowest)
    Assumes that zpool names are unique accross the cluster.
    """
    global xml_resources
    global xml_constraints
    global zpools_seen
    global all_nodes

    all_nodes |= set(nodes)
    zpool = zfs_dataset.split('/')[0]
    mount_point = '/mnt/lustre/'+lustre_service_name

    # Multiple datasets may use the same spool, so ensure that we
    # add each zpool resource only once.
    if zpool in zpools_seen:
        # Sanity check
        if not set(zpools_seen[zpool]).issuperset(set(nodes)):
            sys.stderr.write('zpool "'+zpool+'" already instantiated on incompatible set of nodes\n')
            sys.exit(1)
    elif zpool not in zpools_seen:
        # Add zpool resource
        zpools_seen[zpool] = nodes
        xml_resources += \
'''      <primitive id="'''+zpool+'''" class="ocf" type="zpool" provider="llnl">
        <instance_attributes id="'''+zpool+'''_attributes">
          <nvpair id="'''+zpool+'''_arg1" name="pool" value="'''+zpool+'''"/>
        </instance_attributes>
        <operations>
          <op id="'''+zpool+'''-startup" name="monitor" interval="0" timeout="2min"/>
          <op id="'''+zpool+'''-start" name="start" interval="0" timeout="5min"/>
          <op id="'''+zpool+'''-stop" name="stop" interval="0" timeout="5min"/>
        </operations>
      </primitive>
'''
        # Add zpool resource location constraint
        score = len(nodes) * 10
        for node in nodes:
            xml_constraints += \
'      <rsc_location id="'+zpool+'_loc_'+str(score)+'" rsc="'+zpool+'" node="'+node+'" score="'+str(score)+'"/>\n'
            score -= 10

    # Add lustre resource
    xml_resources += \
'''      <primitive id="'''+lustre_service_name+'''" class="ocf" type="lustre" provider="llnl">
        <instance_attributes id="'''+lustre_service_name+'''_attributes">
          <nvpair id="'''+lustre_service_name+'''_arg1" name="dataset" value="'''+zfs_dataset+'''"/>
          <nvpair id="'''+lustre_service_name+'''_arg2" name="mountpoint" value="'''+mount_point+'''"/>
        </instance_attributes>
        <operations>
          <op id="'''+lustre_service_name+'''-startup" name="monitor" interval="0" timeout="20s"/>
          <op id="'''+lustre_service_name+'''-start" name="start" interval="0" timeout="7min"/>
          <op id="'''+lustre_service_name+'''-stop" name="stop" interval="0" timeout="20min"/>
        </operations>
      </primitive>
'''

    # Add lustre resource location constraint
    # (Is this really necessary?  Maybe the colocation constraint below is enough.)
    score = len(nodes) * 10
    for node in nodes:
        xml_constraints += \
'      <rsc_location id="'+lustre_service_name+'_loc_'+str(score)+'" rsc="'+lustre_service_name+'" node="'+node+'" score="'+str(score)+'"/>\n'
        score -= 10

    # Add zpool and lustre resources' ordering constraint
    # (zpool must be imported before lustre dataset can be mounted)
    xml_constraints += \
'      <rsc_order id="'+lustre_service_name+'_order" first="'+zpool+'" then="'+lustre_service_name+'" kind="Mandatory"/>\n'

    # Add lustre resource colocation with its underlying zpool resource
    # (lustre dataset can only be mounted on the same node as the zpool)
    xml_constraints += \
'      <rsc_colocation id="'+lustre_service_name+'_colocation" rsc="'+lustre_service_name+'" with-rsc="'+zpool+'" score="INFINITY"/>\n'


skip_pattern = re.compile('^\s*(#.*)?\n?$')
def should_skip(line):
    """Returns true is the line contains only a comment or whitespace"""
    if skip_pattern.match(line):
        return True
    return False

# Assume line in ldev looks like this for zfs:
#  jet1   jet2   lquake-MGS0000  zfs:jet1-1/mgs
def process_ldev_line(line):
    args = line.split()
    if len(args) != 4:
        sys.stderr.write('Wrong number of fields in line: '+line+'\n')
        sys.exit(1)
    node1, node2, lustre_service_name, zfs_dataset = args
    nodes = [node1, node2]

    if zfs_dataset[:4] != 'zfs:':
        sys.stderr.write('Fourth field is not prefixed with "zfs:"\n')
        sys.exit(1)
    zfs_dataset = zfs_dataset[4:]

    if 'MGS' in lustre_service_name and lustre_service_name != 'MGS':
        sys.stderr.write('WARNING: Target name "'+lustre_service_name+'" is not valid, using "MGS" instead\n')
        lustre_service_name = 'MGS'

    record_xml(nodes, lustre_service_name, zfs_dataset)

def deduce_cluster_name():
    """Deduce the name of the cluster from the name of one node.

    Strip the trailing digits off the end of the node name, and that probably
    gives us the name of the cluster.  At least at LLNL."""
    global all_nodes

    node = list(all_nodes)[0]

    return node.rstrip('0123456789')

def write_xml_fencing_resource(f, args, node_list):
    f.write('      <primitive class="stonith" id="fence_pm" type="fence_powerman">\n')
    f.write('        <instance_attributes id="fence_pm-instance_attributes">\n')
    if args.fence_static_list:
        nodes_str = ','.join(node_list)
        f.write('          <nvpair id="fence_pm-instance_attributes-hostcheck" name="pcmk_host_check" value="static-list"/>\n')
        f.write('          <nvpair id="fence_pm-instance_attributes-hostlist" name="pcmk_host_list" value="'+nodes_str+'"/>\n')
    f.write('          <nvpair id="fence_pm-instance_attributes-debug" name="debug" value="/tmp/fence_pm.debug"/>\n')
    f.write('          <nvpair id="fence_pm-instance_attributes-verbose" name="verbose" value="1"/>\n')
    f.write('          <nvpair id="fence_pm-instance_attributes-ipaddr" name="ipaddr" value="'+args.fence_ipaddr+'"/>\n')
    f.write('          <nvpair id="fence_pm-instance_attributes-ipport" name="ipport" value="'+args.fence_ipport+'"/>\n')
    f.write('        </instance_attributes>\n')
    f.write('        <operations>\n')
    f.write('          <op id="fence_pm-monitor-interval-10s" interval="10s" name="monitor"/>\n')
    f.write('        </operations>\n')
    f.write('      </primitive>\n')

def write_xml_fencing_constraints(f, node_list):
    """Write out location constraints for the fencing resource

    Since the cluster is in asymmetric mode, we need to explicitly
    add lines to allow the fencing resource to live on any of the
    nodes on the cluster."""

    for i, node in enumerate(node_list):
        f.write('      <rsc_location id="fence_loc_'+str(i)+'" rsc="fence_pm" node="'+node+'" score="10"/>\n')

def write_pacemaker_cib_xml(args):
    global all_nodes

    node_list = list(all_nodes)
    node_list.sort()

    f = args.outfile
    f.write(xml_prefix % (args.admin_epoch, args.cluster_name))
    f.write('    <resources>\n')
    write_xml_fencing_resource(f, args, node_list)
    f.write(xml_resources)
    f.write('    </resources>\n')
    f.write('    <constraints>\n')
    f.write(xml_constraints)
    write_xml_fencing_constraints(f, node_list)
    f.write('    </constraints>\n')
    f.write(xml_postfix)

def main():
    parser = argparse.ArgumentParser(description='Input an ldev.conf file to generate the complimentary Pacemaker cib.xml file.')
    parser.add_argument('infile', nargs='?', type=argparse.FileType('r'),
                        default=sys.stdin,
                        help='ldev.conf file name (default=stdin)')
    parser.add_argument('outfile', nargs='?', type=argparse.FileType('w'),
                        default=sys.stdout,
                        help='Pacemaker cib.xml filename (default=stdout)')
    parser.add_argument('--admin-epoch', type=int, default='1')
    parser.add_argument('--cluster-name',
                        help='Must match corosync "cluster_name" (by default guessed from a random hostname in infile)')
    parser.add_argument('--fence-ipaddr',
                        help='Powerman server IP address (for fence_powerman)')
    parser.add_argument('--fence-ipport', default='10101',
                        help='Powerman server port number (for fence_powerman)')
    parser.add_argument('--fence-static-list', default=False, action="store_true",
                        help='Use static host list for fence_powerman')
    args = parser.parse_args()

    for line in args.infile:
        if should_skip(line):
            continue
        process_ldev_line(line)

    if args.cluster_name is None:
        args.cluster_name = deduce_cluster_name()
    if args.fence_ipaddr is None:
        args.fence_ipaddr = 'e' + args.cluster_name + 'i'

    write_pacemaker_cib_xml(args)

if __name__ == "__main__":
    main()
